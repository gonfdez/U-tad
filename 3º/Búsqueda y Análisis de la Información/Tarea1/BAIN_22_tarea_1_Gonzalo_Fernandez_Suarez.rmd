---
title: "Tarea BAIN 17/03/2022"
author: "Gonzalo Fernández Suárez"
date: "20/3/2022"
output: word_document
---

```{r setup, include=FALSE}
### Por favor asegúrate de tener **tu** ruta al espacio de trabajo R en el chunk siguiente

knitr::opts_chunk$set(echo = TRUE)

# Escogemos el workspace para todos los chunks del R Markdown
knitr::opts_knit$set(root.dir = 'C:/Users/gonef/Desktop/U-tad/3º/Busqueda y Analisis de la informacion/Entrega1_Gonzalo_Fernandez_Suarez')
```

### Es buena práctica cargar al inicio las librerías necesarias para ejecutar el resto del markdown

```{r warning=FALSE, message=FALSE}
library(data.table)
library(stringr)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
```


# Objetivos de este documento RMarkdown y de la tarea

El objetivo de este documento es proporcionar instrucciones sobre la tarea evaluada #1 de la asignatura BAIN, y a la vez, proporcionar un esquema básico de análisis -al menos de los primeros pasos.

Es *crítico* que dispongas de los objetos R contenidos en los diferentes ficheros .rda subidos al Blackboard. (O que verifiques que tus objetos guardados a partir de las clases contienen la misma información).

# Tarea 1

Se trata de que realices un análisis comparativo de los tweets **sobre un tema de tu elección**. Como es evidente, será más interesante si se trata de un tema frecuente ("popular") en el conjunto total de tweets, y quizás también polémico. Estos temas los puedes encontrar en los bi- y tri-gramas obtenidos en clases anteriores, y que puedes replicar a partir del código en BAIN_22_resumen_preproceso_para_tarea_1.rmd.

El proceso de trabajo sugerido (hay infinidad de maneras de abordarlo) es:

1. Extrae del dataframe con todos los tweets aquel subconjunto que contiene el tema (o temas) que te interesen.
2. Crea un corpus con ese subconjunto (y limpia la memoria para evitar colapsos) y añade los metadatos (docvars).
3. Limpia de stopwords y en general de aquellos tokens que no te añaden información o significado.
4. Genera bi- o tri-gramas a partir de los tokens limpiados y muestra la frecuencia relativa de los mismos.

Hasta aquí el resultado supone 5 puntos en el trabajo.

5. Genera los tokens por separado para distintas categorías de tweets (campo Category que se ha introducido como metadato), o por fechas (a partir de la fecha introducida como metadato) y haz una comparación entre ambos a partir de objetos bi- o tri-gramas. 
6. Genera wordclouds comparativos y extrae conclusiones.

Estos dos puntos suponen otros 2 puntos adicionales al trabajo.

7. Realiza un análisis estadístico de corpus o genera tópicos (topicmining) como veremos en clase.
8. Realiza visualizaciones que permitan extraer conclusiones significativas a partir del proceso.

Estos dos puntos suponen otros 2 puntos (hasta 9 sobre 10) del trabajo.

Y un último punto lo otorgaré por la calidad general del documento entregado. 

POR FAVOR LA ENTREGA DEBE SER UN RMARKDOWN *O* COMO ÚNICA ALTERNATIVA, UN SCRIPT .R CON UN .DOC O .PDF O .PPT ASOCIADO

***

# Analisis sociocultural sobre el impacto de atentados terroristas entre 2012-2018 en twitter

Bibliografía:   [www.since911.com](https://since911.com/explore/terrorism-timeline)

## 1 Creación de dataframe con los tweets de interés

Cargamos el objeto creado en clase con los **tweets ya filtrados y limpios** de expresiones regulares y puntuación. Para comprobar que es correcto nos aseguramos de que tenga 1849909 filas (tweets) y 14 columnas.

```{r}
load('tweets_filtrados_data_frame_base_antes_de_corpus.rda')
dim(nuevo_objeto2)

```
Comprobamos también que **no existen URLs ni caracteres no-ascii** (Como emoticonos).

```{r}
grep('noascii', iconv(nuevo_objeto2$content, 
                      from = 'UTF-8', 
                      to = 'ASCII', 
                      sub = 'noascii'))

length(grep('http', nuevo_objeto2$content))/nrow(nuevo_objeto2) #% de tuits con http
```
Ahora mismo la columna "publish_date" contiene información de tipo character (string). Necesitamos pasar **esos datos a tipo fecha** (Date).

```{r}
nuevo_objeto2$publish_date <- as.Date(nuevo_objeto2$publish_date,
                                      format = "%m/%d/%Y")
summary(nuevo_objeto2)

```
Las fechas de los tweets recolectados se encuentran entre **2012-02-06 y 2018-05-23**.

### Extraemos un subconjunto del DataFrame con la información característica para este estudio.

Si nos paramos a mirar el dataframe que tenemos (nuevo_objeto2). Observaremos que las columnas mas relevantes a la hora de **escoger un subconjunto característico** para un estudio son:

* region
* language
* publish_date
  
```{r}
unique(nuevo_objeto2$region)
unique(nuevo_objeto2$language)
length(unique(nuevo_objeto2$publish_date))
```
Debido a filtrados anteriores, la información almacenada en nuestro Dataframe proviene de EEUU y UK. Además todos los tweets se encuentran en Inglés. Por otro lado tenemos tweets de **1364 días distintos**, por lo que escogeré mis subconjuntos de tweets característicos en función de **rangos de tiempo**.

Obtenemos subconjuntos de tweets (Con mas de 4000 tweets) desde 5 días antes de cada atentado hasta 25 días después. Perdiodos de 30 días. La información de cada atentado ha sido obtenida de [www.since911.com](https://since911.com/explore/terrorism-timeline)
```{r, results='hold'}

charlie_hebdo_jan_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-01-02" &
                                       publish_date < "2015-02-02")
dim(charlie_hebdo_jan_2015)

hyper_cacher_jan_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-01-04" &
                                       publish_date < "2015-02-04")
dim(hyper_cacher_jan_2015)

sousse_june_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-06-23" &
                                       publish_date < "2015-07-23")
dim(sousse_june_2015)

bangkok_aug_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-08-13" &
                                       publish_date < "2015-09-13")
dim(bangkok_aug_2015)

ankara_oct_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-10-05" &
                                       publish_date < "2015-11-05")
dim(ankara_oct_2015)

metrojet_flight_oct_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-10-26" &
                                       publish_date < "2015-11-26")
dim(metrojet_flight_oct_2015)

beirut_nov_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-11-07" &
                                       publish_date < "2015-12-07")
dim(beirut_nov_2015)

paris_nov_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-11-08" &
                                       publish_date < "2015-12-08")
dim(paris_nov_2015)

san_berdardino_dec_2015 <-  subset (nuevo_objeto2,
                                       publish_date > "2015-12-17" &
                                       publish_date < "2016-01-17")
dim(san_berdardino_dec_2015)

brussels_march_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-03-17" &
                                       publish_date < "2016-04-17")
dim(brussels_march_2016)

orlando_june_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-06-07" &
                                       publish_date < "2016-07-07")
dim(orlando_june_2016)

jo_cox_june_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-06-11" &
                                       publish_date < "2016-07-11")
dim(jo_cox_june_2016)

dhaka_july_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-06-26" &
                                       publish_date < "2016-07-26")
dim(dhaka_july_2016)

saudi_arabia_july_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-07-01" &
                                       publish_date < "2016-08-01")
dim(saudi_arabia_july_2016)

nice_france_july_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-07-09" &
                                       publish_date < "2016-08-09")
dim(nice_france_july_2016)

berlin_dec_2016 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-12-14" &
                                       publish_date < "2017-01-14")
dim(berlin_dec_2016)

istanbul_jan_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2016-12-26" &
                                       publish_date < "2017-01-26")
dim(istanbul_jan_2017)

westminster_march_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-03-17" &
                                       publish_date < "2017-04-17")
dim(westminster_march_2017)

manchester_may_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-05-17" &
                                       publish_date < "2017-06-17")
dim(manchester_may_2017)

london_bridge_june_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-05-28" &
                                       publish_date < "2017-06-28")
dim(london_bridge_june_2017)

finsbury_june_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-06-14" &
                                       publish_date < "2017-07-14")
dim(finsbury_june_2017)

barcelona_cambrils_aug_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-08-12" &
                                       publish_date < "2017-09-12")
dim(barcelona_cambrils_aug_2017)

parsons_green_sept_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-09-10" &
                                       publish_date < "2017-10-10")
dim(parsons_green_sept_2017)

marseille_oct_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-09-26" &
                                       publish_date < "2017-10-26")
dim(marseille_oct_2017)

mogadishu_oct_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-10-09" &
                                       publish_date < "2017-11-09")
dim(mogadishu_oct_2017)

new_york_oct_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-10-26" &
                                       publish_date < "2017-11-26")
dim(new_york_oct_2017)

sinai_nov_2017 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-11-19" &
                                       publish_date < "2017-12-19")
dim(sinai_nov_2017)

carcasonne_trebes_march_2018 <-  subset (nuevo_objeto2,
                                       publish_date > "2017-03-18" &
                                       publish_date < "2017-04-18")
dim(carcasonne_trebes_march_2018)
```
Como resultado tenemos **28 dataframes** representando los **tweets enviados días antes y después de distintos atentados** en todo el mundo.

Ahora generamos un dataframe que contenga **todos estos subconjuntos** y lo llamaremos *terrorism_attacks*.

```{r}
dataframes = list(ankara_oct_2015,bangkok_aug_2015,barcelona_cambrils_aug_2017,beirut_nov_2015,berlin_dec_2016,brussels_march_2016,carcasonne_trebes_march_2018,charlie_hebdo_jan_2015,dhaka_july_2016,finsbury_june_2017,hyper_cacher_jan_2015,istanbul_jan_2017,jo_cox_june_2016,london_bridge_june_2017,manchester_may_2017,marseille_oct_2017,metrojet_flight_oct_2015,mogadishu_oct_2017,new_york_oct_2017,nice_france_july_2016,orlando_june_2016,paris_nov_2015,parsons_green_sept_2017,san_berdardino_dec_2015,saudi_arabia_july_2016,sinai_nov_2017,sousse_june_2015,westminster_march_2017)

terrorism_attacks_tweets <- Reduce( union_all , dataframes)

dim(terrorism_attacks_tweets)
```

**Guardamos** el objeto (Dataframe previo al corpus QUANTEDA y stop_words).

```{r}
save(terrorism_attacks_tweets,stop_words, file = "terrorism_attacks_tweets.rda")
```

Limpiamos el entorno de trabajo para no ocupar demasiada memoria.
```{r}
rm(list=ls())
```

## 2 Creación de Corpus de Quanteda y agregación de metadatos.

Cargamos el dataframe deseado.
```{r}
load('terrorism_attacks_tweets.rda')
dim(terrorism_attacks_tweets)
summary(terrorism_attacks_tweets)
```

Creamos el **objeto Corpus** a partir del dataframe cargado.

```{r}
terrorism_attacks_corpus <- corpus(terrorism_attacks_tweets$content)
```
### Añadimos los metadatos al Corpus

Los metadatos serán:

* account_category: el tipo de cuenta que envió el tweet.
* retweet: si el tweet es un retweet o no.
* publish_date: la fecha de publicación.


```{r}
docvars(terrorism_attacks_corpus, "Category") <- terrorism_attacks_tweets$account_category
docvars(terrorism_attacks_corpus, "Retweet") <- terrorism_attacks_tweets$retweet
docvars(terrorism_attacks_corpus, "Date_published") <- terrorism_attacks_tweets$publish_date
```

Creamos *tokens_t* para **separar por espacios** las palabras contenidas en en el corpus.

```{r}
tokens_t <- tokens(terrorism_attacks_corpus)
```

## 3 Limpieza de stopwords y tokens no significativos

Stop words recogidas de:

* [www.blog.hubspot.com](https://blog.hubspot.com/marketing/stop-words-seo#:~:text=The%20most%20common%20SEO%20stop,your%2C%20our%2C%20and%20their.)
* [www.countwordsfree.com](https://countwordsfree.com/stopwords)
* [www.gist.github.com/larsyencken](https://gist.github.com/larsyencken/1440509)
* [www.sites.google.com](https://sites.google.com/site/kevinbouge/stopwords-lists)
* [www.algs4.cs.princeton.edu](https://algs4.cs.princeton.edu/35applications/stopwords.txt)
* [www.github.com/stopwords-iso](https://github.com/stopwords-iso/stopwords-en/blob/master/stopwords-en.json)

Con ayuda de Visual Studio Code y [www.textcompare.org](https://www.textcompare.org/text/remove-duplicate-words/) para la correcta identación y eliminado de las palabras duplicadas.


```{r}
mystopwords <- c( stopwords("english"), as.character(seq.int(from = 0, to = 9)),
"im","t","r","rt","|","@","the","in","of","a","for","is","on","ABOUT","ACTUALLY","ALMOST","ALSO","ALTHOUGH","ALWAYS","AM","AN","AND","ANY","ARE","AT","BE","BECAME","BECOME","BUT","BY","CAN","COULD","DID","DO","DOES","EACH","EITHER","ELSE","HAD","HAS","HAVE","HENCE","HOW","I","IF","IT","ITS","JUST","MAY","MAYBE","ME","MIGHT","MINE","MUST","MY","NEITHER","NOR","NOT","OH","OK","WHEN","WHERE","WHEREAS","WHEREVER","WHENEVER","WHETHER","WHICH","WHILE","WHO","WHOM","WHOEVER","WHOSE","WHY","WILL","WITH","WITHIN","WITHOUT","WOULD","YES","YET","YOU","YOUR","us","new","man","above","after","again","against","all","aren","because","been","before","being","below","between","both","cannot","couldn","didn","doesn","doing","don","down","during","few","further","hadn","hasn","haven","having","he","d","ll","s","her","here","hers","herself","him","himself","his","m","ve","into","isn","itself","let","more","most","mustn","myself","no","off","once","only","or","other","ought","our","ours","able","abroad","according","accordingly","across","adj","afterwards","ago","ahead","ain","allow","allows","alone","along","alongside","already","amid","amidst","among","amongst","another","anybody","anyhow","anyone","anything","anyway","anyways","anywhere","apart","appear","appreciate","appropriate","around","aside","ask","asking","associated","available","away","awfully","back","backward","backwards","becomes","becoming","beforehand","begin","behind","believe","beside","besides","best","better","beyond","brief","came","cant","caption","cause","causes","certain","certainly","changes","clearly","cmon","co",".","com","come","comes","concerning","consequently","consider","considering","contain","containing",
"contains","corresponding","course","currently","dare","daren","definitely","described","despite","different","directly","done","downwards","edu","eg","eight","eighty","elsewhere","end","ending","enough","entirely","especially","et","etc","even","ever","evermore","every","everybody","everyone","everything","everywhere","ex","exactly","example","except","fairly","far","farther","fewer","fifth","first","five","followed","following","follows","forever","former","formerly","forth","forward","found","four","furthermore","get","gets","getting","given","gives","go","goes","going","gone","got","gotten","greetings","half","happens","hardly","hello","help","hereafter","hereby","herein","hereupon","hi","hither","hopefully","howbeit","however","hundred","ie","ignored","immediate","inasmuch","inc",".","indeed","indicate","indicated","indicates","inner","inside","insofar","instead","inward","k","keep","keeps","kept","know","known","knows","last","lately","later","latter","latterly","least","less","lest","like","liked","likely","likewise","little","look","looking","looks","low","lower","ltd","made","mainly","make","makes","many","mayn","mean","meantime","meanwhile","merely","mightn","minus","miss","moreover","mostly","mr","mrs","much","name","namely","nd","near","nearly","necessary","need","needn","needs","never","neverf","neverless","nevertheless","next","nine","ninety","nobody","non","none","nonetheless","noone","one","normally","nothing","notwithstanding","novel","now","nowhere","obviously","often","okay","old","ones","onto","opposite","others","otherwise","oughtn","ourselves","out","outside","over","overall","own","particular","particularly","past","per","perhaps","placed","please","plus","possible","presumably","probably","provided","provides","que","quite","qv","rather","rd","re","really","reasonably","recent","recently","regarding","regardless","regards","relatively","respectively","right","round","said","same","saw","say","saying","says","second","secondly","see","seeing","seem","seemed","seeming","seems","seen","self","selves","sensible","sent","serious","seriously","seven","several","shall","shan","she","should","shouldn","since","six","so","some","somebody","someday","somehow","someone","something","sometime","sometimes","somewhat","somewhere","soon","sorry","specified","specify","specifying","still","sub","such","sup","sure","take","taken","taking","tell","tends","th","than","thank","thanks","thanx","that","thats","their","theirs","them","themselves","then","thence","there","thereafter","thereby","therefore","therein","theres","thereupon","these","they","thing","things","think","third","thirty","this","thorough","thoroughly","those","though","three","through","throughout","thru","thus","till","together","too","took","toward","towards","tried","tries","truly","try","trying","twice","two","un","under","underneath","undoing","unfortunately","unless","unlike","unlikely","until","unto","up","upon","upwards",
"use","used","useful","uses","using","usually","v","value","various","versus","very","via","viz","vs","want","wants","was","wasn","way","we","welcome","well","went","were","weren","what","whatever","whence","whereafter","whereby","wherein","whereupon","whichever","whilst","whither","whole","whomever","willing","wish","wonder","won","wouldn","yours","yourself","yourselves","zero","b","e","f","g","h","j","l","n","o","p","q","u","uucp","w","x","y","z","www","amount","bill","bottom","call","computer","con","couldnt","cry","de","describe","detail","due","eleven","empty","fifteen","fifty","fill","find","fire","forty","front","full","give","hasnt","herse","himse","interest","itse”","mill","move","myse”","part","put","show","side","sincere","sixty","system","ten","thick","thin","top","twelve","twenty","abst","accordance","act","added","adopted","affected","affecting","affects","ah","announce","anymore","apparently","approximately","arent","arise","auth","beginning","beginnings","begins","biol","briefly","ca","date","ed","effect","-al","ff","fix","gave","giving","heres","hes","hid","home","id","immediately","importance","important","index","information","invention","itd","keys","kg","km","largely","lets","line","means","mg",
"million","ml","mug","na","nay","necessarily","nos","noted","obtain","obtained","omitted","ord","owing","page","pages","poorly","possibly","potentially","pp","predominantly","present","previously","primarily","promptly","proud","quickly","ran","readily","ref","refs","related","research","resulted","resulting","results","run","sec","section","shed","shes","showed","shown","showns","shows","significant","significantly","similar","similarly","slightly","somethan","specifically","state","states","stop","strongly","substantially","successfully","sufficiently","suggest","thered","thereof","therere","thereto","theyd","theyre","thou","thoughh","thousand","throug","til","tip","ts","ups","usefully","usefulness","vol","vols","wed","whats","wheres","whim","whod","whos","widely","words","world","youd","youre","area","areas","as","asked","asks","backed","backing","backs","began","beings","big","c","case","cases","clear","differ","differently","downed","downing","downs","early","ended","ends","evenly","face","faces","fact","facts","felt","finds","from","fully","furthered","furthering","furthers","general","generally","good","goods","great","greater","greatest","group","grouped","grouping","groups","high","higher","highest","interested","interesting","interests","kind","knew","large","latest","long","longer","longest","making","member","members","men","needed","needing","newer","newest","number","numbers","older","oldest","open","opened","opening","opens","order","ordered","ordering","orders","parted","parting","parts","place","places","point","pointed","pointing","points","presented","presenting","presents","problem","problems","puts","room","rooms","seconds","sees","showing","sides","small","smaller","smallest","thinks","thought","thoughts","to","today","turn","turned","turning","turns","wanted","wanting","ways","wells","work","worked","working","works","year","years","young","younger","youngest","aint","cs","didnt","doesnt","dont","hadnt","havent",
"ill","ive","isnt","itll","shouldnt","theyll","theyve","wasnt","weve","werent","wont","wouldnt","youll","youve","tis","twas","10","39","ableabout","ad","ae","af","ag","ai","amoungst","ao","aq","ar","arpa","au","aw","az","ba","bb","bd","bf","bg","bh","bi","billion","bj","bm","bn","bo","br","bs","bt","buy","bv","bw","bz","cc","cd","cf","cg","ch","ci","ck","cl","click","cm","cn",".","copy","couldve","cr","cu","cv","cx","cy","cz","darent","dear","dj","dk","dm","doubtful","dz","ec","ee","eh","er","es","-","fi","fify","fj","fk","fm","fo","fr","free","fx","ga","gb","gd","ge","gf","gg","gh","gi","gl","gm","gmt","gn","gov","gp","gq","gr","gs","gt","gu","gw","gy","hed","hell","”","”","hk","hm","hn","homepage","howd","howll","hows","hr","ht","htm","html","http","hu","..","ii","il",".","int","io","iq","ir","”","je","jm","jo","join","jp","ke",
"kh","ki","kn","kp","kr","kw","ky","kz","la","lb","lc","length","li","lk","lr","ls","lt","lu","lv","ly","ma","maynt","mc","md","mh","microsoft","mightve","mightnt","mil","mk","mm","mn","mo","mp","mq","ms","msie","mt","mu","mustve","mustnt","mv","mw","mx","”","mz","nc","ne","neednt","net","netscape","nf","ng","ni","nl","-","np","nr","nu","null","nz","om","org","oughtnt","pa","pe","pf","pg","ph","pk","pl","pm","pmid","pn","pr","pt","pw","py","qa","reserved","ring","ro","ru","rw","sa","sb","sc","sd","se","seventy","sg","sh","shant","shell","shouldve","si","site","sj","sk","sl","sm","sn","sr","st","su","sv","sy","sz","tc","td","test","text","tf","tg","thatll","thatve","therell","thereve","tj","tk","tm","tn","tp","tr","trillion","tt","tv","tw","tz","ua","ug","uk","um","uy","uz","va","vc","vg","vi","vn","vu","web","webpage","website","wf","whatd","whatll","whatve","whend","whenll","whens","whered","wherell","wholl","whyd","whyll","whys","width","wouldve","ws","ye",
"yt","yu","za","zm","zr","$","*#*","*tag*","20*","day","week","news","video","time","NA NA")

length(mystopwords)
                  
```
Tenemos **1434 stopwords** del inglés.

Generamos ahora una **matriz** con las **stopwords ya filtradas**.

```{r}
tokens_t_nostop <- tokens_select(tokens_t, 
                             pattern = mystopwords, 
                             selection = "remove")

matriz <- dfm(tokens_t_nostop)
topfeatures(matriz,
            100)
```

Generamos un wordcloud a partir de esta matriz

```{r}
suppressWarnings({ 
  
textplot_wordcloud(matriz,
                   rotation = 0)
})

```
```{r, message=FALSE}
# Generamos un archivo png con el wordcloud
suppressWarnings({ 
  
  png(filename = "wordcloud_tokens_1_palabra.png",
      height = 3000,
      width = 3000)
  
  textplot_wordcloud(matriz,
                     rotation = 0,
                     max_words = 1000)
})

dev.off()
```

Guardamos el **corpus** y los **tokens**.

```{r}
save(  matriz,
       mystopwords,
       tokens_t,
       tokens_t_nostop,
       terrorism_attacks_corpus,
     file = "terrorism_corpus_and_tokens_gram.rda")
```

## 4 Generación de bi-gramas, tri-gramas y cálculo de frecuencias.

Mostramos la **frecuencia** de los tokens almacenados en *terrorism_attacks_tweets*.

Para un **correcto filtrado** añadimos *mystopwords* al objeto *stop_words*.
```{r}
dim(stop_words)
stop_words_add <- data.frame( "word" = mystopwords,
                              "lexicon" = "GFS")
stop_words <- union_all(stop_words, stop_words_add)
dim(stop_words)
```
Pasamos de tener 1149 stop words a 2583.

```{r}

terrorism_tibble <- as_tibble(terrorism_attacks_tweets)

tokens_t_word <- terrorism_tibble %>%
  unnest_tokens(word,
                content,
                to_lower = TRUE)

tokens_t_word_nostop <- tokens_t_word %>%
  anti_join(stop_words)

tokens_t_word_nostop %>%
  count(word ,sort = TRUE) 

suppressWarnings({ 
  
  tokens_t_word_nostop %>%
    count(word, sort = TRUE) %>%
    filter(n > 10000) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(n, word)) +
    geom_col() +
    labs(y = NULL)
})
```
Guardamos *tokens_t_nostop* (Tiene las stopwords ya filtradas) y *stop_words* con nuestras stopwords ya añadidas.
```{r, message=FALSE}
save(tokens_t_nostop, file = "tokens_t_nostop.rda")
save(stop_words, file = "stop_words.rda")
```


Liberamos memoria debido a que los bi-tri-gramas consumen mucha RAM.
```{r message=FALSE}
rm(list=ls())
```


Construimos un bi-grama a partir de *tokens_t_nostop*. Con el una *matriz_bi* para obetener su wordcloud.

```{r}
load("tokens_t_nostop.rda")

bigramas_terrorism <- tokens_ngrams(tokens_t_nostop,                                2)
matriz_bi1 <- dfm(bigramas_terrorism)

suppressWarnings({ 
  
  textplot_wordcloud(matriz_bi1,
                   rotation = 0)
  
  rm(bigramas_terrorism,matriz_bi1)
  })
```

Construimos un tri-grama. También su *matriz_tri* y generamos su wordcloud.

```{r}
trigramas_terrorism <- tokens_ngrams(tokens_t_nostop,
                                  3)
matriz_tri1 <- dfm(trigramas_terrorism)
suppressWarnings({ 
  
  textplot_wordcloud(matriz_tri1,
                   rotation = 0)
  
  rm(trigramas_terrorism,matriz_tri1)
  })
```

## 5 Generamos tokens por separado para distintas fechas (a partir de la fecha introducida como metadato) y los comparamos a partir de objetos bi- o tri-gramas. 

Limpiamos memoria para evitar colapsos.
```{r}
rm(list=ls())
```

Cargamos nuestro objeto *terrorism_attacks_tweets* que contiene los tweets que queremos ya listos para ser procesados. También cargamos *stop_words* actualizado.

```{r}
load("terrorism_attacks_tweets.rda")
load("stop_words.rda")
```

Veamos rápidamente que categorías de cuentas tenemos.
```{r}
table(terrorism_attacks_tweets$account_category)
```

Vamos a generar un dataframe solo con los tweets de cuentas categorizadas como "Fearmonger".

```{r}
terrorism_fearmongers <- as_tibble(terrorism_attacks_tweets) %>%
  filter(account_category == "Fearmonger" )
dim(terrorism_fearmongers)
```

Tenemos 16008 "Fearmongers" en nuestro conjunto de tweets. Generamos los **tokens** y **quitamos las stop words**.

```{r}
tokens_t_fearmongers <- terrorism_fearmongers %>%
  unnest_tokens(word,
                content,
                to_lower = TRUE)

tokens_t_fearmongers_nostop <- tokens_t_fearmongers %>%
  anti_join(stop_words)
```
Hacemos una cuenta de los tweets por día y los mostramos de forma descendente.

```{r}
tokens_t_fearmongers_nostop %>%
  group_by(publish_date) %>%
  count() %>%
  arrange(desc(n))
```

Veamos a lo largo del tiempo como se distribuyen los tweets de los "Fearmongers".

```{r}
tokens_t_fearmongers_nostop %>%
  group_by(publish_date) %>%
  count() %>%
  ggplot(aes(publish_date, n)) +
  geom_line(size = 1) +
  labs(x = NULL, y = "Tweets of Fearmongers")
```

Observamos que solo hay tweets entre Junio de 2015 y Enero de 2016. **Acotemos** el periodo temporal para visualizar mejor la **zona con actividad**.

```{r}
tokens_t_fearmongers_nostop %>%
  filter(  publish_date < "2016-01-01" &
           publish_date > "2015-05-01") %>%
  group_by(publish_date) %>%
  count() %>%
  ggplot(aes(publish_date, n)) +
  geom_line(size = 1) +
  labs(x = NULL, y = "Tweets of Fearmongers")
```

Veamos las **tf-idf** de los términos usados por los "Fearmongers" (n > 800).

```{r}
tokens_t_fearmongers_nostop %>%
  count(word, sort = TRUE) %>%
  filter(n > 800) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = "Words", x = "n")
```

### Comparación de tf-idf de LeftTrolls, RightTrolls y NewsFeed antes/después de 19 Dec 2016

Eliminamos espacio de trabajo para liberar RAM. Y cargamos los objetos necesarios.

```{r}
rm(list=ls())
load("terrorism_attacks_tweets.rda")
load("stop_words.rda")
```

Observemos la diferencia entre las palabras con mayor frecuencia inversa por documento **antes y después del atentado con mas tweets** próximos a su fecha. Como hemos podido observar al principio del estudio, cuando hemos sacado subconjuntos por atentados, el atentado con más tweets cercanos a su fecha es el del (**19 de Diciembre de 2016 en Berlín**)[https://since911.com/explore/terrorism-timeline#jump_time_item_412].

Para ver esta diferencia dejaremos de lado los "Fearmongers", que no tienen una actividad significativa en este periodo.

Vamos a centrarnos en la frecuencia inversa por documento del conjunto **LeftTroll**, **RightTroll** y **NewsFeed**.

#### Comparación LeftTrolls antes y después del 19 Dec 2016

```{r}
terrorism_left_trolls <- as_tibble(terrorism_attacks_tweets) %>%
  filter(account_category == "LeftTroll")

dim(terrorism_left_trolls)
```
```{r, fig.width=12, fig.height=9}
bigrams_tokens_left_trolls <- terrorism_left_trolls %>%
  unnest_tokens(bigram,
                content,
                to_lower = TRUE,
                token = "ngrams", 
                n = 2) 

bigrams_separated <- bigrams_tokens_left_trolls %>%
  separate(bigram, c("word1", "word2"), 
           sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams <- bigrams_united[ , c(4, 11, 14)]
bigrams <- bigrams %>%
  mutate(publish_date, 
         attack = ifelse(publish_date < "2016-12-19", 
                            "before_Berlin_attack", 
                            "after_Berlin_attack"))
suppressWarnings({ 
  
  bigrams %>% 
    group_by(bigram, attack) %>% 
    count() %>%
    filter(n > 200) %>%
    ungroup() %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram, fill = attack)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Frequency", y = NULL) +
    facet_wrap(~attack, ncol = 2, scales = "free")

})

```
#### Comparación RightTrolls antes y después del 19 Dec 2016

```{r}
# Liberamos memoria y cargamos
rm(list=ls())
load("terrorism_attacks_tweets.rda")
load("stop_words.rda")
```
```{r}
terrorism_right_trolls <- as_tibble(terrorism_attacks_tweets) %>%
  filter( account_category == "RightTroll" )

dim(terrorism_right_trolls)
```
```{r, fig.width=12, fig.height=9}
bigrams_tokens_right_trolls <- terrorism_right_trolls %>%
  unnest_tokens(bigram,
                content,
                to_lower = TRUE,
                token = "ngrams", 
                n = 2) 

bigrams_separated <- bigrams_tokens_right_trolls %>%
  separate(bigram, c("word1", "word2"), 
           sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams <- bigrams_united[ , c(4, 11, 14)]
bigrams <- bigrams %>%
  mutate(publish_date, 
         attack = ifelse(publish_date < "2016-12-19", 
                            "before_Berlin_attack", 
                            "after_Berlin_attack"))
suppressWarnings({ 
  
  bigrams %>% 
    group_by(bigram, attack) %>% 
    count() %>%
    filter(n > 200) %>%
    ungroup() %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram, fill = attack)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Frequency", y = NULL) +
    facet_wrap(~attack, ncol = 2, scales = "free")

})

```

#### Comparación NewsFeed antes y después del 19 Dec 2016

```{r}
# Liberamos memoria y cargamos
rm(list=ls())
load("terrorism_attacks_tweets.rda")
load("stop_words.rda")
```
```{r}
terrorism_news_feed <- as_tibble(terrorism_attacks_tweets) %>%
  filter( account_category == "NewsFeed")

dim(terrorism_news_feed)
```
```{r, fig.width=12, fig.height=9}
bigrams_tokens_news_feed <- terrorism_news_feed %>%
  unnest_tokens(bigram,
                content,
                to_lower = TRUE,
                token = "ngrams", 
                n = 2) 

bigrams_separated <- bigrams_tokens_news_feed %>%
  separate(bigram, c("word1", "word2"), 
           sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams <- bigrams_united[ , c(4, 11, 14)]
bigrams <- bigrams %>%
  mutate(publish_date, 
         attack = ifelse(publish_date < "2016-12-19", 
                            "before_Berlin_attack", 
                            "after_Berlin_attack"))
suppressWarnings({ 
  
  bigrams %>% 
    group_by(bigram, attack) %>% 
    count() %>%
    filter(n > 200) %>%
    ungroup() %>%
    mutate(bigram = reorder(bigram, n)) %>%
    ggplot(aes(n, bigram, fill = attack)) +
    geom_col(show.legend = FALSE) +
    labs(x = "Frequency", y = NULL) +
    facet_wrap(~attack, ncol = 2, scales = "free")

})

```




